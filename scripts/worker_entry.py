#!/usr/bin/env python3
"""
Worker Entry Script for Distributed Training
Updated to use JSON config files generated by coordinator
"""

import sys
import argparse
from pathlib import Path

# Add project root to path for imports
sys.path.insert(0, '/workspace/project')

# Import the distributed worker class and config loader
from intellinaut.workers.distributed import DistributedWorker
from intellinaut.config.loader import ConfigLoader

def load_worker_config(worker_id: int, shared_dir: str) -> ConfigLoader:
    """
    Load worker-specific config file generated by coordinator
    
    Args:
        worker_id: Worker ID
        shared_dir: Shared directory path
        
    Returns:
        ConfigLoader with worker config
        
    Raises:
        FileNotFoundError: If worker config file doesn't exist
    """
    config_file = Path(shared_dir) / "worker_configs" / f"worker_{worker_id}_config.json"
    
    if not config_file.exists():
        raise FileNotFoundError(f"Worker config file not found: {config_file}")
    
    config_loader = ConfigLoader(str(config_file))
    print(f"‚úÖ Loaded worker {worker_id} config from: {config_file}")
    return config_loader

def main():
    """Main entry point for distributed worker with config file support"""
    parser = argparse.ArgumentParser(description='Distributed RL Worker with JSON Config')
    
    # Required arguments
    parser.add_argument('--worker_id', type=int, required=True, help='Worker ID')
    parser.add_argument('--shared_dir', type=str, required=True, help='Shared directory')
    
    # Optional arguments (override config file values)
    parser.add_argument('--env', type=str, help='Environment name (overrides config)')
    parser.add_argument('--max_episodes', type=int, help='Max episodes (overrides config)')
    parser.add_argument('--device', type=str, help='Device to use (overrides config)')
    parser.add_argument('--lr', type=float, help='Learning rate (overrides config)')
    parser.add_argument('--sync_frequency', type=int, help='Episodes between sync checks (overrides config)')
    
    args = parser.parse_args()
    
    try:
        # Load worker-specific config file
        config = load_worker_config(args.worker_id, args.shared_dir)
        
        # Extract values from config (with CLI overrides)
        env_name = args.env or config.get('environment.name', 'CartPole-v1')
        max_episodes = args.max_episodes or config.get('training.max_episodes', 1000)
        device = args.device or config.get('training.device', 'cuda:0')
        lr = args.lr or config.get('algorithm.learning_rate', 3e-4)
        sync_frequency = args.sync_frequency or config.get('distributed.sync_frequency', 10)
        
        # Get other config values
        checkpoint_freq = config.get('training.checkpoint_frequency', 50)
        status_update_freq = config.get('training.status_update_frequency', 10)
        
        print(f"üéØ Starting Distributed Worker {args.worker_id}")
        print(f"üìÅ Shared directory: {args.shared_dir}")
        print(f"üéÆ Environment: {env_name}")
        print(f"üìä Max episodes: {max_episodes}")
        print(f"üß† Learning rate: {lr}")
        print(f"üîÑ Sync frequency: every {sync_frequency} episodes")
        
        # Create distributed worker
        worker = DistributedWorker(
            worker_id=args.worker_id,
            shared_dir=args.shared_dir,
            max_episodes=max_episodes,
            checkpoint_frequency=checkpoint_freq,
            status_update_frequency=status_update_freq
        )
        
        # Run distributed training with config values
        worker.run_distributed(
            env_name=env_name,
            device=device,
            lr=lr,
            sync_check_frequency=sync_frequency
        )
        
    except FileNotFoundError as e:
        print(f"‚ùå Config Error: {e}")
        print(f"üí° Make sure coordinator has generated config files first")
        sys.exit(1)
        
    except Exception as e:
        print(f"‚ùå Worker Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()