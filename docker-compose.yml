# Complete AI Mission Control - Distributed Training + Development
# Architecture: 1 Coordinator + N Workers + Development Container
# Uses JSON configs with Bayesian Optimization

services:
  # Distributed Training Services
  coordinator:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-coordinator
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=""
      - NUM_WORKERS=${NUM_WORKERS:-2}
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
      # Mount config files
      - ./configs:/workspace/configs
    working_dir: /workspace/project
    command: >
      bash -c "
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/coordinator_entry.py
      --shared_dir /workspace/distributed_shared
      --config /workspace/configs/${CONFIG_FILE:-cartpole_distributed.json}
      --num_workers ${NUM_WORKERS:-2}
      --check_interval 30
      "
    networks:
      - rl-training
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "test", "-f", "/workspace/distributed_shared/coordinator_status.json"]
      interval: 10s
      timeout: 5s
      retries: 3

  worker-0:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-worker-0
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
    working_dir: /workspace/project
    # Updated to use config files instead of CLI args
    command: >
      bash -c "
      echo 'Waiting for coordinator to generate configs...' &&
      while [ ! -f '/workspace/distributed_shared/worker_configs/worker_0_config.json' ]; do
        echo 'Config not ready, waiting 10s...' &&
        sleep 10;
      done &&
      echo 'Config found, starting worker...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/worker_entry.py 
      --worker_id 0 
      --shared_dir /workspace/distributed_shared
      "
    networks:
      - rl-training
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  worker-1:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-worker-1
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
    working_dir: /workspace/project
    # Updated to use config files instead of CLI args
    command: >
      bash -c "
      echo 'Waiting for coordinator to generate configs...' &&
      while [ ! -f '/workspace/distributed_shared/worker_configs/worker_1_config.json' ]; do
        echo 'Config not ready, waiting 10s...' &&
        sleep 10;
      done &&
      echo 'Config found, starting worker...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/worker_entry.py
      --worker_id 1
      --shared_dir /workspace/distributed_shared
      "
    networks:
      - rl-training
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  worker-2:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-worker-2
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
    working_dir: /workspace/project
    command: >
      bash -c "
      echo 'Waiting for coordinator to generate configs...' &&
      while [ ! -f '/workspace/distributed_shared/worker_configs/worker_2_config.json' ]; do
        echo 'Config not ready, waiting 10s...' &&
        sleep 10;
      done &&
      echo 'Config found, starting worker...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/worker_entry.py
      --worker_id 2
      --shared_dir /workspace/distributed_shared
      "
    networks:
      - rl-training
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  worker-3:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-worker-3
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
    working_dir: /workspace/project
    command: >
      bash -c "
      echo 'Waiting for coordinator to generate configs...' &&
      while [ ! -f '/workspace/distributed_shared/worker_configs/worker_3_config.json' ]; do
        echo 'Config not ready, waiting 10s...' &&
        sleep 10;
      done &&
      echo 'Config found, starting worker...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/worker_entry.py
      --worker_id 3
      --shared_dir /workspace/distributed_shared
      "
    networks:
      - rl-training
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  worker-4:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-worker-4
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
    working_dir: /workspace/project
    command: >
      bash -c "
      echo 'Waiting for coordinator to generate configs...' &&
      while [ ! -f '/workspace/distributed_shared/worker_configs/worker_4_config.json' ]; do
        echo 'Config not ready, waiting 10s...' &&
        sleep 10;
      done &&
      echo 'Config found, starting worker...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/worker_entry.py
      --worker_id 4
      --shared_dir /workspace/distributed_shared
      "
    networks:
      - rl-training
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  worker-5:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-worker-5
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
    working_dir: /workspace/project
    command: >
      bash -c "
      echo 'Waiting for coordinator to generate configs...' &&
      while [ ! -f '/workspace/distributed_shared/worker_configs/worker_5_config.json' ]; do
        echo 'Config not ready, waiting 10s...' &&
        sleep 10;
      done &&
      echo 'Config found, starting worker...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/worker_entry.py
      --worker_id 5
      --shared_dir /workspace/distributed_shared
      "
    networks:
      - rl-training
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  worker-6:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-worker-6
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
    working_dir: /workspace/project
    command: >
      bash -c "
      echo 'Waiting for coordinator to generate configs...' &&
      while [ ! -f '/workspace/distributed_shared/worker_configs/worker_6_config.json' ]; do
        echo 'Config not ready, waiting 10s...' &&
        sleep 10;
      done &&
      echo 'Config found, starting worker...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/worker_entry.py
      --worker_id 6
      --shared_dir /workspace/distributed_shared
      "
    networks:
      - rl-training
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  worker-7:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-worker-7
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
    working_dir: /workspace/project
    command: >
      bash -c "
      echo 'Waiting for coordinator to generate configs...' &&
      while [ ! -f '/workspace/distributed_shared/worker_configs/worker_7_config.json' ]; do
        echo 'Config not ready, waiting 10s...' &&
        sleep 10;
      done &&
      echo 'Config found, starting worker...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/worker_entry.py
      --worker_id 7
      --shared_dir /workspace/distributed_shared
      "
    networks:
      - rl-training
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  worker-8:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-worker-8
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
    working_dir: /workspace/project
    command: >
      bash -c "
      echo 'Waiting for coordinator to generate configs...' &&
      while [ ! -f '/workspace/distributed_shared/worker_configs/worker_8_config.json' ]; do
        echo 'Config not ready, waiting 10s...' &&
        sleep 10;
      done &&
      echo 'Config found, starting worker...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/worker_entry.py
      --worker_id 8
      --shared_dir /workspace/distributed_shared
      "
    networks:
      - rl-training
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  worker-9:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-worker-9
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
    working_dir: /workspace/project
    command: >
      bash -c "
      echo 'Waiting for coordinator to generate configs...' &&
      while [ ! -f '/workspace/distributed_shared/worker_configs/worker_9_config.json' ]; do
        echo 'Config not ready, waiting 10s...' &&
        sleep 10;
      done &&
      echo 'Config found, starting worker...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/worker_entry.py
      --worker_id 9
      --shared_dir /workspace/distributed_shared
      "
    networks:
      - rl-training
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  worker-10:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-worker-10
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
    working_dir: /workspace/project
    command: >
      bash -c "
      echo 'Waiting for coordinator to generate configs...' &&
      while [ ! -f '/workspace/distributed_shared/worker_configs/worker_10_config.json' ]; do
        echo 'Config not ready, waiting 10s...' &&
        sleep 10;
      done &&
      echo 'Config found, starting worker...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/worker_entry.py
      --worker_id 10
      --shared_dir /workspace/distributed_shared
      "
    networks:
      - rl-training
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  worker-11:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rl-worker-11
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ./logs:/workspace/logs
      - .:/workspace/project
    working_dir: /workspace/project
    command: >
      bash -c "
      echo 'Waiting for coordinator to generate configs...' &&
      while [ ! -f '/workspace/distributed_shared/worker_configs/worker_11_config.json' ]; do
        echo 'Config not ready, waiting 10s...' &&
        sleep 10;
      done &&
      echo 'Config found, starting worker...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      python scripts/worker_entry.py
      --worker_id 11
      --shared_dir /workspace/distributed_shared
      "
    networks:
      - rl-training
    depends_on:
      coordinator:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  # Development & Monitoring Services
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ai-mc-jupyter
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./:/workspace/project
      - ./distributed_shared:/workspace/distributed_shared
      - ./configs:/workspace/configs
      - ai_mc_experiments:/workspace/experiments
      - ai_mc_logs:/workspace/logs
      - ai_mc_models:/workspace/models
    ports:
      - "8080:8080"  # Jupyter Lab
      - "8888:8888"  # Alternative Jupyter port
    working_dir: /workspace/project
    command: >
      bash -c "
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      jupyter lab 
      --ip=0.0.0.0 
      --port=8080 
      --no-browser 
      --allow-root
      --notebook-dir=/workspace
      "
    networks:
      - rl-training
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  tensorboard:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ai-mc-tensorboard
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
    volumes:
      - ./distributed_shared:/workspace/distributed_shared
      - ai_mc_experiments:/workspace/experiments
      - ai_mc_logs:/workspace/logs
    ports:
      - "6006:6006"  # TensorBoard
    working_dir: /workspace
    command: >
      bash -c "
      echo 'Waiting for log directories...' &&
      sleep 15 &&
      echo 'Starting TensorBoard...' &&
      source /opt/miniforge/etc/profile.d/conda.sh &&
      conda activate automl &&
      if [ -d '/workspace/distributed_shared/worker_logs' ]; then
        tensorboard --logdir=/workspace/distributed_shared/worker_logs --host=0.0.0.0 --port=6006 --reload_interval=30;
      else
        tensorboard --logdir=/workspace/distributed_shared --host=0.0.0.0 --port=6006 --reload_interval=30;
      fi
      "
    networks:
      - rl-training
    depends_on:
      - coordinator

  # Development Container (for debugging, manual work)
  dev:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ai-mc-dev
    entrypoint: ""
    environment:
      - PYTHONPATH=/workspace/project
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./:/workspace/project
      - ./distributed_shared:/workspace/distributed_shared
      - ./configs:/workspace/configs
      - ai_mc_experiments:/workspace/experiments
      - ai_mc_logs:/workspace/logs
      - ai_mc_models:/workspace/models
    working_dir: /workspace/project
    command: tail -f /dev/null  # Keeps container running for exec access
    stdin_open: true
    tty: true
    networks:
      - rl-training
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

networks:
  rl-training:
    driver: bridge

volumes:
  ai_mc_experiments:
  ai_mc_logs:
  ai_mc_models: